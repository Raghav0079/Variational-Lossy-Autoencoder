\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{enumitem} % For proper enumeration
\setlist[itemize]{leftmargin=*} % Reset left margin for itemize globally
\setlist[enumerate]{leftmargin=1cm} % Set left margin for enumerate globally
\usepackage{float} % For precise table placement
\usepackage{multirow} % For multirow in tables
\usepackage{tikz} % For tikzpicture diagrams
\usetikzlibrary{positioning} % For node positioning (right=of, above=of, etc.)
\usepackage{amsmath} % For math environments like E[...] and DKL
\usepackage{booktabs} % For improved table formatting
\usepackage{amsfonts} % For \mathbb used in the equation
\date{} % This disables the date in the output PDF
\title{Variational Lossy Autoencoder (VLAE)}
\author{Raghav Mishra}

\begin{document}

\maketitle

\section{First Pass: Core Concepts}

\begin{itemize}
    \item The VLAE aims to extract \textbf{useful features} from observed data for downstream tasks such as classification.
    \item The \textbf{Variational Lossy Autoencoder (VLAE)} combines \textbf{VAEs} with \textbf{neural autoregressive models} (e.g., PixelCNN) to improve generative modeling performance.
    \item VLAE leverages autoregressive models as both the \textbf{prior distribution} $p(z)$ and the \textbf{decoding distribution} $p(x|z)$ to greatly improve generative modeling performance of VAEs.
    \item The model is designed to be a \textbf{lossy compressor} of observed data. The analysis identifies the conditions under which the latent code ($z$) in a VAE should be used for autoencoding.
    \item VLAE has the appealing properties of \textbf{controllable representation learning} and improved density estimation performance, allowing control over what the global latent code can learn.
    \item VLAE is \textbf{slower at generation} due to the sequential nature of the autoregressive model used in the decoder $p(x|z)$.
\end{itemize}

\section{Second Pass: Key Results}

\begin{itemize}
    \item Achieved new state-of-the-art results on \textbf{MNIST}, \textbf{OMNIGLOT}, and \textbf{Caltech-101 Silhouettes} density estimation tasks, as well as competitive results on CIFAR10.
    \item The conditional distribution $p(x|z)$ is implemented with a small receptive-field \textbf{PixelCNN} (e.g., 6 layers of masked convolution with filter size 3).
    \item Reported marginal \textbf{Negative Log-Likelihood (NLL)} is estimated using Importance Sampling with 4096 samples.
    \item For the Statically Binarized MNIST model, the converged expected DKL is $\mathbb{E}[\mathrm{DKL}(q(z|x)\|p(z))] = 13.3~\text{nats} = 19.2~\text{bits}$.
    \item VLAE can learn a \textbf{lossier compression} than a VAE with a regular factorized conditional distribution.
\end{itemize}

\begin{table}[H]
\centering
\caption{Statically Binarized MNIST: Model NLL Test Results (nats/dim)}
\label{tab:mnist_static}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{NLL Test} \\
\midrule
Normalizing flows (Rezende \& Mohamed, 2015) & 85.10 \\
DRAW (Gregor et al., 2015) & $<$ 80.97 \\
Discrete VAE (Rolfe, 2016) & 81.01 \\
PixelRNN (van den Oord et al., 2016a) & 79.20 \\
IAF VAE (Kingma et al., 2016) & 79.88 \\
AF VAE & 79.30 \\
\textbf{VLAE} & \textbf{79.03} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{Dynamically Binarized MNIST: Model NLL Test Results (nats/dim)}
    \label{tab:mnist_dynamic}
    \begin{tabular}{lc}
    \toprule
    \textbf{Model} & \textbf{NLL Test} \\
    \midrule
    Convolutional VAE + HVI (Salimans et al., 2014) & 81.94 \\
    DLGM 2hl + IWAE (Burda et al., 2015a) & 82.90 \\
    Discrete VAE (Rolfe, 2016) & 80.04 \\
    LVAE (Kaae Sønderby et al., 2016) & 81.74 \\
    DRAW + VGP (Tran et al., 2015) & $<$ 79.88 \\
    IAF VAE (Kingma et al., 2016) & 79.10 \\
    Unconditional Decoder & 87.55 \\
    \textbf{VLAE} & \textbf{78.53} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{OMNIGLOT: Model NLL Test Results (nats/dim)}
    \label{tab:omniglot}
    \begin{tabular}{lc}
    \toprule
    \textbf{Model} & \textbf{NLL Test} \\
    \midrule
    VAE (Burda et al., 2015a) & 106.31 \\
    IWAE (Burda et al., 2015a) & 103.38 \\
    RBM (500 hidden) (Burda et al., 2015b) & 100.46 \\
    DRAW (Gregor et al., 2015) & $<$ 96.50 \\
    Conv DRAW (Gregor et al., 2016) & $<$ 91.00 \\
    Unconditional Decoder & 95.02 \\
    \textbf{VLAE} & \textbf{90.98} \\
    \textbf{VLAE (fine-tuned)} & \textbf{89.83} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{Caltech-101 Silhouettes: Model NLL Test Results (nats/dim)}
    \label{tab:caltech}
    \begin{tabular}{lc}
    \toprule
    \textbf{Model} & \textbf{NLL Test} \\
    \midrule
    RWS SBN (Bornschein et al., 2014) & 113.3 \\
    RBM (Cho et al., 2011) & 107.8 \\
    NAIS NADE (Du et al., 2015) & 100.0 \\
    Discrete VAE (Rolfe, 2016) & 97.6 \\
    SpARN (Goessling et al., 2015) & 88.48 \\
    Unconditional Decoder & 89.26 \\
    \textbf{VLAE} & \textbf{77.36} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{itemize}
    \item VLAE was applied to the \textbf{CIFAR10} dataset of natural images.
    \item VLAE models attain new state-of-the-art performance among other variationally trained latent-variable models.
    \item The DenseNet VLAE model also outperforms most other tractable likelihood models including Gated PixelCNN and PixelRNN and has results only slightly worse than the then state-of-the-art PixelCNN++.
\end{itemize}
\begin{table}[H]
    \centering
    \caption{CIFAR10: Test set bits/dim for various models. Likelihood for VLAE is approximated with 512 importance samples.}
    \label{tab:cifar10}
    \begin{tabular}{lc}
    \toprule
    \textbf{Method} & \textbf{bits/dim} \\
    \midrule
    \multicolumn{2}{l}{\textit{Tractable likelihood models}} \\
    \midrule
    Uniform distribution \cite{van2016pixel} & 8.00 \\
    Multivariate Gaussian \cite{van2016pixel} & 4.70 \\
    NICE \cite{dinh2014nice} & 4.48 \\
    Deep GMMs \cite{oord2014deep} & 4.00 \\
    Real NVP \cite{dinh2016density} & 3.49 \\
    PixelCNN \cite{van2016pixel} & 3.14 \\
    Gated PixelCNN \cite{van2016conditional} & 3.03 \\
    PixelRNN \cite{van2016pixel} & 3.00 \\
    PixelCNN++ \cite{salimans2017pixelcnn++} & 2.92 \\
    \midrule
    \multicolumn{2}{l}{\textit{Variationally trained latent-variable models}} \\
    \midrule
    Deep Diffusion \cite{sohl2015deep} & 5.40 \\
    Convolutional DRAW \cite{gregor2016towards} & 3.58 \\
    ResNet VAE with IAF \cite{kingma2016improved} & 3.11 \\
    ResNet VLAE & 3.04 \\
    DenseNet VLAE & \textbf{2.95} \\
    \bottomrule
    \end{tabular}
\end{table}

\section{Third Pass: Model Theory}
\subsection{Introduction: Variational Inference and Coding}
\begin{itemize}
    \item The ultimate goal is for the model to uncover and untangle those causal sources of variations that are of interest.
    \item An autoregressive model of the data may achieve the same log-likelihood as a variational autoencoder (VAE).
    \item Notably, an autoregressive model has no stochastic latent variables at all.
    \item A VAE is frequently interpreted as a \textbf{regularized autoencoder}.
    \item Let $x$ be the observed variables and $z$ the latent variables. Let $p(x, z)$ be the parametric model of their joint distribution, called the \textbf{generative model}. Given a dataset $X = \{x_1, \ldots, x_N\}$, we wish to perform maximum likelihood learning of its parameters:
\begin{equation}
    \log p(X) = \sum_{i=1}^N \log p(x^{(i)}),
\end{equation}
where $p(x^{(i)}) = \int p(x^{(i)}, z) \, dz$ is typically intractable for complex models, motivating the use of variational inference.
    \item Let $q(z|x)$ be a parametric inference model (also called the encoder or recognition model) defined over the latent variables. We optimize the \textbf{variational lower bound (ELBO)} on the marginal log-likelihood of each observation $x$:
\begin{equation}
    \log p(x) \geq \mathbb{E}_{q(z|x)} \left[ \log p(x, z) - \log q(z|x) \right] = \mathcal{L}(x; \theta)
\end{equation}
where $\theta$ indicates the parameters of both the generative model $p$ and the inference model $q$.
    \item The ELBO $\mathcal{L}(x; \theta)$ can be re-arranged:
    \begin{equation}
        \begin{aligned}
            \mathcal{L}(x; \theta) &= \mathbb{E}_{q(z|x)} \left[ \log p(x, z) - \log q(z|x) \right] \\
            &= \mathbb{E}_{q(z|x)} \left[ \log p(x|z) \right] - \mathrm{DKL}\left(q(z|x)\|p(z)\right)
        \end{aligned}
    \end{equation}
    \item A more powerful $p(x|z)$ will make VAE’s marginal generative distribution $p(x)$ more expressive.
    \item The issue of latent code collapse: when a very powerful autoregressive decoder (like an RNN) is used for $p(x|z)$, it can in theory model the entire data distribution $p(x)$ without using $z$. Consequently, $z$ is sometimes completely ignored, and the model regresses to be a standard unconditional autoregressive distribution.
    \item The goal of designing an efficient coding protocol is to minimize the expected code length of communicating $x$.
    \item \textbf{Naive Two-Part Coding}: VAE implies a two-part code: $p(z)$ (essence/structure) and $p(x|z)$ (modeling error/deviation). The expected code length is:
    \begin{equation}
        C_{\text{naive}}(x) = \mathbb{E}_{x \sim \text{data},\, z \sim q(z|x)} \left[ -\log p(z) - \log p(x|z) \right]
    \end{equation}
    \item In this scheme, information that can be modeled locally by the decoding distribution $p(x|z)$ without access to $z$ will be encoded locally, and only the remainder (long-range dependency) will be encoded in $z$. This relates to the concept of \textbf{lossy compression} achieved by VLAE.
\end{itemize}

\subsection{VLAE: Model Definition and Architecture}
\begin{itemize}
    \item VLAE presents two complementary classes of improvements to VAE that utilize autoregressive models to explicitly control representation learning and improve density estimation.
    \item **Conditional Decoder $p(x|z)$ with Restricted Receptive Field:**
    \begin{itemize}
        \item Instead of conditioning the full decoder on $z$, VLAE restricts the receptive field of the autoregressive decoder to a small local window $x_{\text{WindowAround}(i)}$ around the pixel $x_i$: $p(x_i | x_{<i}, z) \approx p(x_i | x_{\text{WindowAround}(i)}, z)$.
        \item Since $x_{\text{WindowAround}(i)}$ is smaller than $x_{<i}$, $p(x|z)$ won't be able to represent arbitrarily complex distributions over $x$ without dependence on $z$, as not all distributions over $x$ admit such factorizations with limited receptive fields.
        \item \textbf{Feature Learning}: Local statistics of 2D images like texture will likely be modeled completely by the small local window. In contrast, global structural information of images (like shapes of objects) is a long-range dependency that \textbf{must} be communicated through the latent code $z$, thus forcing $z$ to learn useful global representations.
    \end{itemize}
    \item **Autoregressive Flow (AF) Prior $p(z)$:**
    \begin{itemize}
        \item The VAE objective encourages the approximate posterior $q(z|x)$ to match the prior $p(z)$, leading to inefficient latent codes if $q(z|x)$ is too expressive. The mismatch between $q(z|x)$ and $p(z)$ can be exploited to construct a lossy code.
        \item VLAE parametrizes the prior distribution $p(z)$ using an \textbf{Autoregressive Flow (AF)} from some simple noise source (e.g., spherical Gaussian), which can theoretically reduce inefficiency in Bits-Back coding.
        \item \textbf{AF definition}: For an autoregressive flow $f$, a continuous noise source $\epsilon$ is transformed into the latent code $z$: $z = f(\epsilon)$. Assuming the density function for the noise source is $u(\epsilon)$, the log-prior is $\log p(z) = \log u(\epsilon) - \log \left| \det \frac{d f^{-1}}{d z} \right|$. (The sign in the determinant log is often opposite to the noted $\log \det \frac{d f}{d \epsilon}$ depending on convention).
        \item \textbf{IAF/AF Equivalence}: The AF prior $p(z)$ is equivalent to using an **Inverse Autoregressive Flow (IAF)** to model the approximate posterior $q(z|x)$.
        \item \textbf{IAF form}: $\epsilon_i = (z_i - \mu_i(z_{<i})) / \sigma_i(z_{<i})$, where $\mu_i(\cdot)$ and $\sigma_i(\cdot)$ are transformations generated autoregressively.
        \item \textbf{Benefit}: Using an AF prior makes the model more expressive without incurring extra cost during training time for the ELBO computation (under the expectation of $z \sim q(z|x)$) because the transformation complexity is the same as that of an IAF posterior.
        \item \textbf{Density Estimation}: The AF prior $p(z)$ is more expressive than a simple factorized prior, allowing the model to better match the aggregated posterior $q(z) = \int q(z|x) p_{\text{data}}(x) \, dx$ and thus improve density estimation performance.
        \item IAF posterior has a shorter decoder path $p(x|z)$ whereas AF prior has a deeper decoder path $p(x|f(\epsilon))$.
    \end{itemize}
\end{itemize}

% \nocite{*} % Uncomment this if you want all cited references to appear even if not explicitly referenced in the main text body (assuming a bibliography is added).

\end{document}